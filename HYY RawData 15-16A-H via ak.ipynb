{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<CENTER><img src=\"../../images/ATLASOD.gif\" style=\"width:50%\"></CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to rediscover the Higgs boson yourself!\n",
    "This notebook uses ATLAS Open Data http://opendata.atlas.cern to show you the steps to rediscover the Higgs boson yourself!\n",
    "\n",
    "ATLAS Open Data provides open access to proton-proton collision data at the LHC for educational purposes. ATLAS Open Data resources are ideal for high-school, undergraduate and postgraduate students.\n",
    "\n",
    "Notebooks are web applications that allow you to create and share documents that can contain for example:\n",
    "1. live code\n",
    "2. visualisations\n",
    "3. narrative text\n",
    "\n",
    "This analysis loosely follows the discovery of the Higgs boson by ATLAS https://arxiv.org/pdf/1207.7214.pdf (mostly Section 5 and 5.1)\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "1. rediscover the Higgs boson yourself!\n",
    "2. know some general principles of a particle physics analysis\n",
    "\n",
    "Feynman diagram pictures are borrowed from our friends at https://www.particlezoo.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<CENTER><img src=\"images/feynman_diagrams/Hyy_feynman.png\" style=\"width:40%\"></CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='contents'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents: \n",
    "\n",
    "[Running a Jupyter notebook](#running) <br />\n",
    "[First time setup on your computer (no need on mybinder)](#setup_computer) <br />\n",
    "[To setup everytime](#setup_everytime) <br />\n",
    "[Lumi, fraction, file path](#fraction) <br />\n",
    "[Samples](#samples) <br />\n",
    "[Changing a cut](#changing_cut) <br />\n",
    "[Applying a cut](#applying_cut) <br />\n",
    "[Plotting](#plotting) <br />\n",
    "[What can you do to explore this analysis?](#going_further) <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='running'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Jupyter notebook\n",
    "\n",
    "To run the whole Jupyter notebook, in the top menu click Cell -> Run All.\n",
    "\n",
    "To propagate a change you've made to a piece of code, click Cell -> Run All Below.\n",
    "\n",
    "You can also run a single code cell, by clicking Cell -> Run Cells, or using the keyboard shortcut Shift+Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup_computer'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First time setup on your computer (no need on mybinder)\n",
    "This first cell only needs to be run the first time you open this notebook on your computer. \n",
    "\n",
    "If you close Jupyter and re-open on the same computer, you won't need to run this first cell again.\n",
    "\n",
    "If you open on binder, you don't need to run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install --upgrade --user pip # update the pip package installer\n",
    "#!{sys.executable} -m pip install -U numpy==2.0.0 pandas==2.2.2 uproot==5.3.9 matplotlib==3.9.0 lmfit==1.3.1 awkward-pandas==2023.8.0 aiohttp==3.9.5 requests==2.32.3 --user # install required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup_everytime'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To setup everytime\n",
    "Cell -> Run All Below\n",
    "\n",
    "to be done every time you re-open this notebook\n",
    "\n",
    "We're going to be using a number of tools to help us:\n",
    "* uproot: lets us read .root files typically used in particle physics into data formats used in python\n",
    "* pandas: lets us store data as dataframes, a format widely used in python\n",
    "* numpy: provides numerical calculations such as histogramming\n",
    "* matplotlib: common tool for making plots, figures, images, visualisations\n",
    "* lmfit: tool for statistical fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install lmfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install awkward-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uproot # for reading .root files\n",
    "import awkward as ak \n",
    "import time # to measure time to analyse\n",
    "import math # for mathematical functions such as square root\n",
    "import numpy as np # # for numerical calculations such as histogramming\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from matplotlib.ticker import MaxNLocator,AutoMinorLocator # for minor ticks;\n",
    "from lmfit.models import PolynomialModel, GaussianModel # for the signal and background fits\n",
    "import requests # for HTTP access\n",
    "import aiohttp # HTTP client support\n",
    "import vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check tree content of old and new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108492747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ScaleFactor_PILEUP',\n",
       " 'mcWeight',\n",
       " 'xsec',\n",
       " 'trigE',\n",
       " 'trigM',\n",
       " 'ScaleFactor_BTAG',\n",
       " 'jet_n',\n",
       " 'jet_pt',\n",
       " 'jet_eta',\n",
       " 'jet_phi',\n",
       " 'jet_e',\n",
       " 'jet_DL1d77_isBtagged',\n",
       " 'jet_jvt',\n",
       " 'largeRJet_n',\n",
       " 'largeRJet_pt',\n",
       " 'largeRJet_eta',\n",
       " 'largeRJet_phi',\n",
       " 'largeRJet_e',\n",
       " 'largeRJet_m',\n",
       " 'largeRJet_D2',\n",
       " 'ScaleFactor_ELE',\n",
       " 'ScaleFactor_MUON',\n",
       " 'lep_n',\n",
       " 'lep_type',\n",
       " 'lep_pt',\n",
       " 'lep_eta',\n",
       " 'lep_phi',\n",
       " 'lep_e',\n",
       " 'lep_charge',\n",
       " 'lep_ptvarcone30',\n",
       " 'lep_topoetcone20',\n",
       " 'lep_z0',\n",
       " 'lep_d0',\n",
       " 'lep_d0sig',\n",
       " 'lep_isTight',\n",
       " 'lep_isTightID',\n",
       " 'lep_isTightIso',\n",
       " 'ScaleFactor_PHOTON',\n",
       " 'photon_n',\n",
       " 'photon_pt',\n",
       " 'photon_eta',\n",
       " 'photon_phi',\n",
       " 'photon_e',\n",
       " 'photon_ptcone20',\n",
       " 'photon_topoetcone40',\n",
       " 'photon_isTight',\n",
       " 'photon_isTightID',\n",
       " 'photon_isTightIso',\n",
       " 'ScaleFactor_TAU',\n",
       " 'tau_n',\n",
       " 'tau_pt',\n",
       " 'tau_eta',\n",
       " 'tau_phi',\n",
       " 'tau_e',\n",
       " 'tau_charge',\n",
       " 'tau_nTracks',\n",
       " 'tau_isTight',\n",
       " 'tau_RNNJetScore',\n",
       " 'tau_RNNEleScore',\n",
       " 'met',\n",
       " 'met_phi',\n",
       " 'met_mpx',\n",
       " 'met_mpy']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new file\n",
    "pname = '/project/etp1/dkoch/ATLASOpenData/ntuples-data-samples/data15_allyear.root'\n",
    "pname = '/project/etp1/dkoch/ATLASOpenData/ntuples-data-samples/data16_allyear_G.root'\n",
    "tree = uproot.open(pname+':analysis')\n",
    "\n",
    "print(tree.num_entries)\n",
    "\n",
    "tree.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data15_periodD', 'data15_periodG', 'data16_periodA', 'data16_periodD', 'data16_periodG', 'data16_periodL', 'data15_periodE', 'data15_periodH', 'data16_periodB', 'data16_periodE', 'data16_PeriodI', 'data15_periodF', 'data15_periodJ', 'data16_periodC', 'data16_periodF', 'data16_periodK']\n",
      "Total number of entries: 4998873\n"
     ]
    }
   ],
   "source": [
    "import uproot\n",
    "\n",
    "# Define the path to the directory and the list of samples\n",
    "tuple_path = \"/project/etp1/dkoch/ATLASOpenData/ntuples-data-samples/\"\n",
    "tuple_path = \"/project/etp1/dkoch/ATLASOpenData/GamGam/Data/\"\n",
    "\n",
    "\n",
    "\n",
    "samples_list = ['data15_allyear', 'data16_allyear_A', 'data16_allyear_B', \n",
    "                'data16_allyear_C', 'data16_allyear_D', 'data16_allyear_E', \n",
    "                'data16_allyear_F', 'data16_allyear_G', 'data16_allyear_H']\n",
    "\n",
    "\n",
    "samples_list = [\n",
    "    'data15_periodD', 'data15_periodG', 'data16_periodA', 'data16_periodD', 'data16_periodG', 'data16_periodL',\n",
    "    'data15_periodE', 'data15_periodH', 'data16_periodB', 'data16_periodE', 'data16_PeriodI',\n",
    "    'data15_periodF', 'data15_periodJ', 'data16_periodC', 'data16_periodF', 'data16_periodK'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "print(samples_list)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a variable to store the total number of entries\n",
    "total_entries = 0\n",
    "\n",
    "# Iterate through each sample in the list\n",
    "for sample in samples_list:\n",
    "    # Construct the full path to the .root file\n",
    "    pname = f\"{tuple_path}{sample}.root\"\n",
    "    \n",
    "    try:\n",
    "        # Open the .root file and access the 'analysis' tree\n",
    "        with uproot.open(pname + ':analysis') as tree:\n",
    "            # Add the number of entries in the current tree to the total\n",
    "            total_entries += tree.num_entries\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pname}: {e}\")\n",
    "\n",
    "# Print the total number of entries across all samples\n",
    "print(f\"Total number of entries: {total_entries}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "myt = tree.iterate([\"photon_n\",\"photon_pt\",\"photon_eta\",\"photon_phi\",\"photon_e\",\n",
    "                            \"photon_isTightID\",\"photon_ptcone20\"], # add more variables here if you want to use them\n",
    "                           library=\"ak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = next(myt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "no field named 'query'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphoton_n>=2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/awkward/highlevel.py:1240\u001b[0m, in \u001b[0;36mArray.__getattr__\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1236\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile trying to get field \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhere\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m, an exception \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1237\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moccurred:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(err)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1238\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno field named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhere\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: no field named 'query'"
     ]
    }
   ],
   "source": [
    "data = data.query('photon_n>=2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>[[False, False, False],\n",
       " [True, True, False, False, False],\n",
       " [False, False, False, False],\n",
       " [False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [True, False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [True, True],\n",
       " [True, False, False, False, False, False],\n",
       " [False, False],\n",
       " ...,\n",
       " [False, False],\n",
       " [True, True, False],\n",
       " [True, False, False, False, False, False, False],\n",
       " [False, False],\n",
       " [False, False],\n",
       " [False, False, False, False],\n",
       " [False, False, False, False],\n",
       " [False, False, False],\n",
       " [False, False, False]]\n",
       "--------------------------------------------------\n",
       "type: 289333 * var * bool</pre>"
      ],
      "text/plain": [
       "<Array [[False, False, False], ..., [False, ...]] type='289333 * var * bool'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.photon_e\n",
    "data[\"photon_isTightID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fraction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lumi, fraction, file path\n",
    "\n",
    "General definitions of luminosity, fraction of data used, where to access the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lumi = 0.5 # fb-1 # data_A only\n",
    "#lumi = 1.9 # fb-1 # data_B only\n",
    "#lumi = 2.9 # fb-1 # data_C only\n",
    "#lumi = 4.7 # fb-1 # data_D only\n",
    "lumi = 10 # fb-1 # data_A,data_B,data_C,data_D\n",
    "MeV = 0.001\n",
    "GeV = 1\n",
    "fraction = 1 # reduce this is you want the code to run quicker\n",
    "\n",
    "#tuple_path = \"Input/GamGam/Data/\" # local \n",
    "tuple_path = \"/project/etp1/dkoch/ATLASOpenData/ntuples-data-samples/\"\n",
    "tuple_path = \"/project/etp1/dkoch/ATLASOpenData/GamGam/Data/\"\n",
    "#tuple_path = \"https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/GamGam/Data/\" # web address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='samples'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples\n",
    "\n",
    "Samples to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#samples_list = ['data_A','data_B','data_C','data_D'] # add if you want more data\n",
    "samples_list = ['data15_allyear', 'data16_allyear_A', 'data16_allyear_B', \n",
    "                'data16_allyear_C', 'data16_allyear_D', 'data16_allyear_E', \n",
    "                'data16_allyear_F', 'data16_allyear_G', 'data16_allyear_H' ]\n",
    "#samples_list = ['data15_allyear']  \n",
    "samples_list = [\n",
    "    'data15_periodD', 'data15_periodG', 'data16_periodA', 'data16_periodD', 'data16_periodG', 'data16_periodL',\n",
    "    'data15_periodE', 'data15_periodH', 'data16_periodB', 'data16_periodE', 'data16_PeriodI',\n",
    "    'data15_periodF', 'data15_periodJ', 'data16_periodC', 'data16_periodF', 'data16_periodK'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to get data from files\n",
    "\n",
    "The datasets used in this notebook have already been filtered to include at least 2 photons per event, so that processing is quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_from_files():\n",
    "\n",
    "\n",
    "    frames = [] # define empty list to hold data\n",
    "    for val in samples_list: # loop over each file\n",
    "        fileString = tuple_path+val+\".root\" # file name to open\n",
    "        temp = read_file(fileString,val) # call the function read_file defined below\n",
    "        frames.append(temp) # append dataframe returned from read_file to list of dataframes\n",
    "    data = ak.concatenate(frames) # concatenatelist of awkward arrays\n",
    "    \n",
    "    return data # return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to calculate diphoton invariant mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_myy(photon_pt,photon_eta,photon_phi,photon_e):\n",
    "    # construct awkward 4-vector array\n",
    "    p4 = vector.awk(ak.zip(dict(pt=photon_pt, eta=photon_eta, phi=photon_phi, E=photon_e)))\n",
    "    # calculate invariant mass of first 4 leptons\n",
    "    # [:, i] selects the i-th lepton in each event\n",
    "    # .M calculates the invariant mass\n",
    "    return (p4[:, 0] + p4[:, 1]).M * MeV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='changing_cut'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cut on photon reconstruction quality\n",
    "# paper: \"Photon candidates are required to pass identification criteria\"\n",
    "def cut_photon_reconstruction(photon_isTightID):\n",
    "    # Keep only the events where the first two photons are both True\n",
    "    return (photon_isTightID[:, 0] == True) & ak.any(photon_isTightID[:, 1] == True)\n",
    "\n",
    "    \n",
    "# Cut on Transverse momentum\n",
    "# paper: \"The leading (sub-leading) photon candidate is required to have ET > 40 GeV (30 GeV)\"\n",
    "def cut_photon_pt(photon_pt):\n",
    "\n",
    "    return (photon_pt[:,0] >40 ) & (photon_pt[:,1]>30 )\n",
    "\n",
    "# Cut on energy isolation\n",
    "# paper: \"Photon candidates are required to have an isolation transverse energy of less than 4 GeV\"\n",
    "def cut_isolation_pt(photon_ptcone20):\n",
    "# want to keep events where isolation eT<4000 MeV\n",
    "    return (photon_ptcone20[:,0]<4) & (photon_ptcone20[:,1]<4 )\n",
    "\n",
    "# Cut on pseudorapidity in barrel/end-cap transition region\n",
    "# paper: \"excluding the calorimeter barrel/end-cap transition region 1.37 < |η| < 1.52\"\n",
    "def cut_photon_eta_transition(photon_eta):\n",
    "# want to keep events where modulus of photon_eta is outside the range 1.37 to 1.52\n",
    "    return ((np.abs(photon_eta[:,0])>1.52) | (np.abs(photon_eta[:,0])<1.37)) & ((np.abs(photon_eta[:,1])>1.52) | (np.abs(photon_eta[:,1])<1.37))\n",
    "\n",
    "\n",
    "def cut_n_photon(photon_n):\n",
    "# >=2\n",
    "    return photon_n>=2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='applying_cut'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Applying a cut \n",
    "\n",
    "If you add a cut: Cell -> Run All Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uproot\n",
    "import awkward as ak\n",
    "\n",
    "# Initialize global cutflow dictionary\n",
    "global_cutflow = {\n",
    "    \"Before Cuts\": 0,\n",
    "    \"After photon reconstruction cut\": 0,\n",
    "    \"After photon pt cut\": 0,\n",
    "    \"After photon isolation cut\": 0,\n",
    "    \"After photon eta transition cut\": 0\n",
    "}\n",
    "\n",
    "def read_file(path, sample, fraction=1.0):\n",
    "    start = time.time()\n",
    "    print(f\"\\tProcessing: {sample}\")\n",
    "    data_all = []\n",
    "\n",
    "    # Initialize local cutflow dictionary\n",
    "    local_cutflow = {\n",
    "        \"Before Cuts\": 0,\n",
    "        \"After photon reconstruction cut\": 0,\n",
    "        \"After photon pt cut\": 0,\n",
    "        \"After photon isolation cut\": 0,\n",
    "        \"After photon eta transition cut\": 0\n",
    "    }\n",
    "\n",
    "    with uproot.open(path + \":analysis\") as tree:\n",
    "        numevents = tree.num_entries\n",
    "        for data in tree.iterate([\"photon_pt\", \"photon_eta\", \"photon_phi\", \"photon_e\",\n",
    "                                  \"photon_isTightID\", \"photon_ptcone20\"],\n",
    "                                 library=\"ak\",\n",
    "                                 entry_stop=int(numevents*fraction)):\n",
    "            \n",
    "            local_cutflow[\"Before Cuts\"] += len(data)\n",
    "            global_cutflow[\"Before Cuts\"] += len(data)\n",
    "\n",
    "            data = data[cut_photon_reconstruction(data.photon_isTightID)]\n",
    "            local_cutflow[\"After photon reconstruction cut\"] += len(data)\n",
    "            global_cutflow[\"After photon reconstruction cut\"] += len(data)\n",
    "\n",
    "            data = data[cut_photon_pt(data.photon_pt)]\n",
    "            local_cutflow[\"After photon pt cut\"] += len(data)\n",
    "            global_cutflow[\"After photon pt cut\"] += len(data)\n",
    "\n",
    "            data = data[cut_isolation_pt(data.photon_ptcone20)]\n",
    "            local_cutflow[\"After photon isolation cut\"] += len(data)\n",
    "            global_cutflow[\"After photon isolation cut\"] += len(data)\n",
    "\n",
    "            data = data[cut_photon_eta_transition(data.photon_eta)]\n",
    "            local_cutflow[\"After photon eta transition cut\"] += len(data)\n",
    "            global_cutflow[\"After photon eta transition cut\"] += len(data)\n",
    "\n",
    "            data['myy'] = calc_myy(data.photon_pt, data.photon_eta, data.photon_phi, data.photon_e)\n",
    "            data_all.append(data)\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "            print(f\"\\t\\t nIn: {local_cutflow['Before Cuts']},\\t nOut: \\t{len(data)}\\t in {round(elapsed,1)}s\")\n",
    "\n",
    "    final_data = ak.concatenate(data_all, highlevel=True)\n",
    "\n",
    "    print(\"\\nLocal Cutflow Summary:\")\n",
    "    print(f\"{'Cut':<35}{'Entries':<15}{'Percentage Reduction':<25}\")\n",
    "    print(\"-\" * 75)\n",
    "    previous_entries = local_cutflow[\"Before Cuts\"]\n",
    "    for cut, entries in local_cutflow.items():\n",
    "        if cut == \"Before Cuts\":\n",
    "            print(f\"{cut:<35}{entries:<15}-\")\n",
    "        else:\n",
    "            reduction = (previous_entries - entries) / previous_entries * 100\n",
    "            print(f\"{cut:<35}{entries:<15}{reduction:.2f}%\")\n",
    "        previous_entries = entries\n",
    "    total_reduction = (local_cutflow[\"Before Cuts\"] - local_cutflow[\"After photon eta transition cut\"]) / local_cutflow[\"Before Cuts\"] * 100\n",
    "    print(\"-\" * 75)\n",
    "    print(f\"{'Total entries after all cuts':<35}{local_cutflow['After photon eta transition cut']:<15}{total_reduction:.2f}%\")\n",
    "\n",
    "    return final_data\n",
    "\n",
    "def print_global_cutflow():\n",
    "    print(\"\\nGlobal Cutflow Summary:\")\n",
    "    print(f\"{'Cut':<35}{'Entries':<15}{'Percentage Reduction':<25}\")\n",
    "    print(\"-\" * 75)\n",
    "    previous_entries = global_cutflow[\"Before Cuts\"]\n",
    "    for cut, entries in global_cutflow.items():\n",
    "        if cut == \"Before Cuts\":\n",
    "            print(f\"{cut:<35}{entries:<15}-\")\n",
    "        else:\n",
    "            reduction = (previous_entries - entries) / previous_entries * 100\n",
    "            print(f\"{cut:<35}{entries:<15}{reduction:.2f}%\")\n",
    "        previous_entries = entries\n",
    "    total_reduction = (global_cutflow[\"Before Cuts\"] - global_cutflow[\"After photon eta transition cut\"]) / global_cutflow[\"Before Cuts\"] * 100\n",
    "    print(\"-\" * 75)\n",
    "    print(f\"{'Total entries after all cuts':<35}{global_cutflow['After photon eta transition cut']:<15}{total_reduction:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the processing happens (this will take some minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tProcessing: data15_periodD\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#pname = '/project/etp1/dkoch/ATLASOpenData/ntuples-data-samples/data15_allyear.root'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# time at start of whole processing\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mget_data_from_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# process all files\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#data = read_file_new(pname) #\u001b[39;00m\n\u001b[1;32m      5\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start \u001b[38;5;66;03m# time after whole processing\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m, in \u001b[0;36mget_data_from_files\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m samples_list: \u001b[38;5;66;03m# loop over each file\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     fileString \u001b[38;5;241m=\u001b[39m tuple_path\u001b[38;5;241m+\u001b[39mval\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.root\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# file name to open\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileString\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# call the function read_file defined below\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(temp) \u001b[38;5;66;03m# append dataframe returned from read_file to list of dataframes\u001b[39;00m\n\u001b[1;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m ak\u001b[38;5;241m.\u001b[39mconcatenate(frames) \u001b[38;5;66;03m# concatenatelist of awkward arrays\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 56\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(path, sample, fraction)\u001b[0m\n\u001b[1;32m     53\u001b[0m local_cutflow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter photon eta transition cut\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[1;32m     54\u001b[0m global_cutflow[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter photon eta transition cut\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[0;32m---> 56\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmyy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_myy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphoton_pt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphoton_eta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphoton_phi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphoton_e\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m data_all\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[1;32m     59\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m, in \u001b[0;36mcalc_myy\u001b[0;34m(photon_pt, photon_eta, photon_phi, photon_e)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_myy\u001b[39m(photon_pt,photon_eta,photon_phi,photon_e):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# construct awkward 4-vector array\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     p4 \u001b[38;5;241m=\u001b[39m \u001b[43mvector\u001b[49m\u001b[38;5;241m.\u001b[39mawk(ak\u001b[38;5;241m.\u001b[39mzip(\u001b[38;5;28mdict\u001b[39m(pt\u001b[38;5;241m=\u001b[39mphoton_pt, eta\u001b[38;5;241m=\u001b[39mphoton_eta, phi\u001b[38;5;241m=\u001b[39mphoton_phi, E\u001b[38;5;241m=\u001b[39mphoton_e)))\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# calculate invariant mass of first 4 leptons\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# [:, i] selects the i-th lepton in each event\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# .M calculates the invariant mass\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (p4[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m p4[:, \u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mM \u001b[38;5;241m*\u001b[39m MeV\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vector' is not defined"
     ]
    }
   ],
   "source": [
    "#pname = '/project/etp1/dkoch/ATLASOpenData/ntuples-data-samples/data15_allyear.root'\n",
    "start = time.time() # time at start of whole processing\n",
    "data = get_data_from_files() # process all files\n",
    "#data = read_file_new(pname) #\n",
    "elapsed = time.time() - start # time after whole processing\n",
    "print(\"Time taken: \"+str(round(elapsed,1))+\"s\") # print total time taken to process every file\n",
    "print_global_cutflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global Cutflow Summary:\n",
      "Cut                                Entries        Percentage Reduction     \n",
      "---------------------------------------------------------------------------\n",
      "Before Cuts                        4998873        -\n",
      "After photon reconstruction cut    488229         90.23%\n",
      "After photon pt cut                259806         46.79%\n",
      "After photon isolation cut         157854         39.24%\n",
      "After photon eta transition cut    156974         0.56%\n",
      "---------------------------------------------------------------------------\n",
      "Total entries after all cuts       156974         96.86%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plotting'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "If you only want a make a change in the plot: Cell -> Run All Below\n",
    "\n",
    "Define function to plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from awkward import Array, to_list\n",
    "from lmfit.models import PolynomialModel, GaussianModel\n",
    "from matplotlib.ticker import AutoMinorLocator, MaxNLocator\n",
    "\n",
    "def plot_data(data):\n",
    "    # Ensure data is an awkward array\n",
    "    myy_data = Array(data['myy']) if not isinstance(data['myy'], Array) else data['myy']\n",
    "    \n",
    "    # Check for NaN values\n",
    "    myy_np = to_list(myy_data)\n",
    "    if np.any(np.isnan(myy_np)):\n",
    "        raise ValueError(\"Input data contains NaN values!\")\n",
    "\n",
    "    xmin = 100  # GeV\n",
    "    xmax = 160  # GeV\n",
    "    step_size = 2  # GeV\n",
    "    \n",
    "    bin_edges = np.arange(start=xmin, stop=xmax + step_size, step=step_size)\n",
    "    bin_centres = np.arange(start=xmin + step_size / 2, stop=xmax + step_size / 2, step=step_size)\n",
    "    \n",
    "    print(\"bin_centres is:\", bin_centres)\n",
    "\n",
    "    # Convert awkward array to a numpy array for histogram\n",
    "    data_x, _ = np.histogram(myy_np, bins=bin_edges)  # histogram the data\n",
    "\n",
    "    print('data myy is:', myy_np)\n",
    "    print(\"data_x is:\", data_x)\n",
    "    \n",
    "    # Check for NaN values in histogram data\n",
    "    if np.any(np.isnan(data_x)):\n",
    "        raise ValueError(\"Histogram data contains NaN values!\")\n",
    "\n",
    "    data_x_errors = np.sqrt(data_x)  # statistical error on the data\n",
    "\n",
    "    # Avoid fitting if all data_x is zero (which leads to NaNs)\n",
    "    if np.all(data_x == 0):\n",
    "        raise ValueError(\"All histogram counts are zero; cannot fit a model.\")\n",
    "\n",
    "    # Fit models\n",
    "    polynomial_mod = PolynomialModel(4)  # 4th order polynomial\n",
    "    gaussian_mod = GaussianModel()  # Gaussian\n",
    "\n",
    "    # Set initial guesses for the parameters of the polynomial model\n",
    "    pars = polynomial_mod.guess(data_x, x=bin_centres, c0=data_x.max(), c1=20000, c2=1000, c3=0, c4=0)\n",
    "    print('data_x.max() is:', data_x.max())\n",
    "\n",
    "    # Set initial gues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['myy'] #invriant masses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function to plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='going_further'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can you do to explore this analysis?\n",
    "\n",
    "* Increase the fraction of data used in '[Lumi, fraction, file path](#fraction)'\n",
    "* Use data_B, data_C and data_D in '[Samples](#samples)'\n",
    "* Check how many events are being thrown away by each cut in '[Applying a cut](#applying_cut)'\n",
    "* Add more cuts from the [Higgs discovery paper](https://www.sciencedirect.com/science/article/pii/S037026931200857X#se0090) in '[Changing a cut](#changing_cut)' and '[Applying a cut](#applying_cut)'\n",
    "* Find the reduced chi-squared for the fit in '[Plotting](#plotting)'\n",
    "* Find the mean of the fitted Gaussian in '[Plotting](#plotting)'\n",
    "* Find the width of the fitted Gaussian in '[Plotting](#plotting)'\n",
    "* Try different initial guesses for the parameters of the fit in '[Plotting](#plotting)'\n",
    "* Try different functions for the fit in '[Plotting](#plotting)'\n",
    "* Your idea!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data):   \n",
    "    # Define your range, binning, and calculate the histogram\n",
    "    xmin = 100 # GeV\n",
    "    xmax = 160 # GeV\n",
    "    step_size = 2 # GeV\n",
    "    \n",
    "    bin_edges = np.arange(start=xmin, stop=xmax + step_size, step=step_size)\n",
    "    bin_centres = np.arange(start=xmin + step_size/2, stop=xmax + step_size/2, step=step_size)\n",
    "\n",
    "    data_x, _ = np.histogram(data['myy'], bins=bin_edges)\n",
    "    data_x_errors = np.sqrt(data_x)  # Statistical error on the data\n",
    "\n",
    "    # Define the polynomial and Gaussian models\n",
    "    polynomial_mod = PolynomialModel(4)  # 4th order polynomial\n",
    "    gaussian_mod = GaussianModel()\n",
    "\n",
    "    # Initial parameter guesses\n",
    "    pars = polynomial_mod.guess(data_x, x=bin_centres, c0=data_x.max(), c1=0, c2=0, c3=0, c4=0)\n",
    "    pars += gaussian_mod.guess(data_x, x=bin_centres, amplitude=1600, center=125, sigma=2)\n",
    "\n",
    "    # Combine the models\n",
    "    model = polynomial_mod + gaussian_mod\n",
    "\n",
    "    # Fit the model to the data\n",
    "    out = model.fit(data_x, pars, x=bin_centres, weights=1/data_x_errors)\n",
    "\n",
    "    # Check if fit was successful\n",
    "    if out.success:\n",
    "        print(\"Fit was successful!\")\n",
    "    else:\n",
    "        print(\"Fit failed!\")\n",
    "\n",
    "    # Extract and print fit parameters\n",
    "    params_dict = out.params.valuesdict()  # Get the fitted parameter values as a dictionary\n",
    "    for param, value in params_dict.items():\n",
    "        print(f\"{param} = {value}\")\n",
    "    \n",
    "    # Optional: return the parameters for further use\n",
    "    return params_dict\n",
    "\n",
    "    # ... (rest of your plotting code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, out = plot_data(data)\n",
    "print(\"Fitted parameters: \", params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import awkward as ak\n",
    "\n",
    "# Assuming 'data_filtered' is your filtered DataFrame from the previous step\n",
    "# First, flatten the 'photon_pt' lists into a 1D array using Awkward\n",
    "all_photon_pt = ak.flatten(data['photon_e']).to_list()\n",
    "\n",
    "# Now plot the distribution of photon transverse momenta\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_photon_pt, bins=60, range = (100,160), color='b', alpha=0.7)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title('Distribution of Photon Energy', fontsize=14)\n",
    "plt.xlabel('Photon_e [GeV]', fontsize=12)\n",
    "plt.ylabel('Number of Events', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import awkward as ak\n",
    "\n",
    "# Assuming 'data_filtered' is your filtered DataFrame from the previous step\n",
    "# Flatten the 'photon_pt' lists into 1D arrays using Awkward\n",
    "all_photon_pt1 = ak.flatten(data['photon_pt'][0]).to_list()\n",
    "all_photon_pt2 = ak.flatten(data['photon_pt'][1]).to_list()\n",
    "\n",
    "# Create a figure with two subplots (1 row, 2 columns)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for the first photon\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, first plot\n",
    "plt.hist(all_photon_pt1, bins=60, range=(0, 250), color='b', alpha=0.7)\n",
    "plt.title('Photon 1: Transverse Momentum', fontsize=14)\n",
    "plt.xlabel('Photon pT [GeV]', fontsize=12)\n",
    "plt.ylabel('%', fontsize=12)\n",
    "\n",
    "# Plot for the second photon\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, second plot\n",
    "plt.hist(all_photon_pt2, bins=60, range=(0, 250), color='r', alpha=0.7)\n",
    "plt.title('Photon 2: Transverse Momentum', fontsize=14)\n",
    "plt.xlabel('Photon pT [GeV]', fontsize=12)\n",
    "plt.ylabel('%', fontsize=12)\n",
    "\n",
    "# Adjust the layout to make sure plots don't overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import awkward as ak\n",
    "\n",
    "# Assuming 'data_filtered' is your filtered DataFrame from the previous step\n",
    "# Flatten the 'photon_pt' lists into 1D arrays using Awkward\n",
    "all_photon_pt1 = ak.flatten(data['photon_eta'][0]).to_list()\n",
    "all_photon_pt2 = ak.flatten(data['photon_eta'][1]).to_list()\n",
    "\n",
    "# Create a figure with two subplots (1 row, 2 columns)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for the first photon\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, first plot\n",
    "plt.hist(all_photon_pt1, bins=8, range=(-4, 4), color='b', alpha=0.7)\n",
    "plt.title('Photon 1: Eta', fontsize=14)\n",
    "plt.xlabel('Photon eta  ', fontsize=12)\n",
    "plt.ylabel('%', fontsize=12)\n",
    "\n",
    "# Plot for the second photon\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, second plot\n",
    "plt.hist(all_photon_pt2, bins=8, range=(-4, 4), color='r', alpha=0.7)\n",
    "plt.title('Photon 2: Eta', fontsize=14)\n",
    "plt.xlabel('Photon eta  ]', fontsize=12)\n",
    "plt.ylabel('%', fontsize=12)\n",
    "\n",
    "# Adjust the layout to make sure plots don't overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import awkward as ak\n",
    "\n",
    "# Assuming 'data_filtered' is your filtered DataFrame from the previous step\n",
    "# Flatten the 'photon_pt' lists into 1D arrays using Awkward\n",
    "all_photon_pt1 = ak.flatten(data['photon_ptcone20'][0]).to_list()\n",
    "all_photon_pt2 = ak.flatten(data['photon_ptcone20'][1]).to_list()\n",
    "\n",
    "# Create a figure with two subplots (1 row, 2 columns)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for the first photon\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, first plot\n",
    "plt.hist(all_photon_pt1, bins=10, range=(0, 10), color='b', alpha=0.7)\n",
    "plt.title('Photon 1: ptcone20', fontsize=14)\n",
    "plt.xlabel('ptcone20 value  ', fontsize=12)\n",
    "plt.ylabel('%', fontsize=12)\n",
    "\n",
    "# Plot for the second photon\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, second plot\n",
    "plt.hist(all_photon_pt2, bins=10, range=(0, 10), color='r', alpha=0.7)\n",
    "plt.title('Photon 2: ptcone20', fontsize=14)\n",
    "plt.xlabel('ptcone20 value ', fontsize=12)\n",
    "plt.ylabel('%', fontsize=12)\n",
    "\n",
    "# Adjust the layout to make sure plots don't overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import awkward as ak\n",
    "\n",
    "# Assuming 'data_filtered' is your filtered DataFrame from the previous step\n",
    "# Flatten the 'photon_pt' lists into 1D arrays using Awkward\n",
    "all_photon_pt1 = ak.flatten(data['photon_phi'][0]).to_list()\n",
    "all_photon_pt2 = ak.flatten(data['photon_phi'][1]).to_list()\n",
    "\n",
    "# Create a figure with two subplots (1 row, 2 columns)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot for the first photon\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, first plot\n",
    "plt.hist(all_photon_pt1, bins=10, range=(0, 10), color='b', alpha=0.7)\n",
    "plt.title('Photon 1: phi', fontsize=14)\n",
    "plt.xlabel('Photon phi  ', fontsize=12)\n",
    "plt.ylabel('%', fontsize=12)\n",
    "\n",
    "# Plot for the second photon\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, second plot\n",
    "plt.hist(all_photon_pt2, bins=10, range=(0, 10), color='r', alpha=0.7)\n",
    "plt.title('Photon 2: phi', fontsize=14)\n",
    "plt.xlabel('Photon phi  ]', fontsize=12)\n",
    "plt.ylabel('%', fontsize=12)\n",
    "\n",
    "# Adjust the layout to make sure plots don't overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['photon_isTightID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
