{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<CENTER><img src=\"../../images/ATLASOD.gif\" style=\"width:50%\"></CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to rediscover the Higgs boson yourself!\n",
    "This notebook uses ATLAS Open Data http://opendata.atlas.cern to show you the steps to rediscover the Higgs boson yourself!\n",
    "\n",
    "ATLAS Open Data provides open access to proton-proton collision data at the LHC for educational purposes. ATLAS Open Data resources are ideal for high-school, undergraduate and postgraduate students.\n",
    "\n",
    "Notebooks are web applications that allow you to create and share documents that can contain for example:\n",
    "1. live code\n",
    "2. visualisations\n",
    "3. narrative text\n",
    "\n",
    "This analysis loosely follows the discovery of the Higgs boson by ATLAS https://arxiv.org/pdf/1207.7214.pdf (mostly Section 5 and 5.1)\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "1. rediscover the Higgs boson yourself!\n",
    "2. know some general principles of a particle physics analysis\n",
    "\n",
    "Feynman diagram pictures are borrowed from our friends at https://www.particlezoo.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<CENTER><img src=\"images/feynman_diagrams/Hyy_feynman.png\" style=\"width:40%\"></CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='contents'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents: \n",
    "\n",
    "[Running a Jupyter notebook](#running) <br />\n",
    "[First time setup on your computer (no need on mybinder)](#setup_computer) <br />\n",
    "[To setup everytime](#setup_everytime) <br />\n",
    "[Lumi, fraction, file path](#fraction) <br />\n",
    "[Samples](#samples) <br />\n",
    "[Changing a cut](#changing_cut) <br />\n",
    "[Applying a cut](#applying_cut) <br />\n",
    "[Plotting](#plotting) <br />\n",
    "[What can you do to explore this analysis?](#going_further) <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='running'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Jupyter notebook\n",
    "\n",
    "To run the whole Jupyter notebook, in the top menu click Cell -> Run All.\n",
    "\n",
    "To propagate a change you've made to a piece of code, click Cell -> Run All Below.\n",
    "\n",
    "You can also run a single code cell, by clicking Cell -> Run Cells, or using the keyboard shortcut Shift+Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup_computer'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First time setup on your computer (no need on mybinder)\n",
    "This first cell only needs to be run the first time you open this notebook on your computer. \n",
    "\n",
    "If you close Jupyter and re-open on the same computer, you won't need to run this first cell again.\n",
    "\n",
    "If you open on binder, you don't need to run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install --upgrade --user pip # update the pip package installer\n",
    "#!{sys.executable} -m pip install -U numpy==2.0.0 pandas==2.2.2 uproot==5.3.9 matplotlib==3.9.0 lmfit==1.3.1 awkward-pandas==2023.8.0 aiohttp==3.9.5 requests==2.32.3 --user # install required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup_everytime'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To setup everytime\n",
    "Cell -> Run All Below\n",
    "\n",
    "to be done every time you re-open this notebook\n",
    "\n",
    "We're going to be using a number of tools to help us:\n",
    "* uproot: lets us read .root files typically used in particle physics into data formats used in python\n",
    "* pandas: lets us store data as dataframes, a format widely used in python\n",
    "* numpy: provides numerical calculations such as histogramming\n",
    "* matplotlib: common tool for making plots, figures, images, visualisations\n",
    "* lmfit: tool for statistical fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: lmfit in /home/i/I.Kuscu/.local/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: asteval>=1.0 in /home/i/I.Kuscu/.local/lib/python3.11/site-packages (from lmfit) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.19 in /home/i/I.Kuscu/.local/lib/python3.11/site-packages (from lmfit) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6 in /software/opt/el_9/x86_64/python/3.11-2023.09/lib/python3.11/site-packages (from lmfit) (1.11.2)\n",
      "Requirement already satisfied: uncertainties>=3.2.2 in /home/i/I.Kuscu/.local/lib/python3.11/site-packages (from lmfit) (3.2.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in /software/opt/el_9/x86_64/python/3.11-2023.09/lib/python3.11/site-packages (from lmfit) (0.3.7)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install lmfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: awkward-pandas in /home/i/I.Kuscu/.local/lib/python3.11/site-packages (2023.8.0)\n",
      "Requirement already satisfied: awkward>=2.0.0 in /home/i/I.Kuscu/.local/lib/python3.11/site-packages (from awkward-pandas) (2.6.7)\n",
      "Requirement already satisfied: pandas>=1.2 in /software/opt/el_9/x86_64/python/3.11-2023.09/lib/python3.11/site-packages (from awkward-pandas) (2.1.0)\n",
      "Requirement already satisfied: awkward-cpp==37 in /home/i/I.Kuscu/.local/lib/python3.11/site-packages (from awkward>=2.0.0->awkward-pandas) (37)\n",
      "Requirement already satisfied: fsspec>=2022.11.0 in /software/opt/el_9/x86_64/python/3.11-2023.09/lib/python3.11/site-packages (from awkward>=2.0.0->awkward-pandas) (2023.9.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /software/opt/el_9/x86_64/python/3.11-2023.09/lib/python3.11/site-packages (from awkward>=2.0.0->awkward-pandas) (6.8.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/i/I.Kuscu/.local/lib/python3.11/site-packages (from awkward>=2.0.0->awkward-pandas) (1.26.4)\n",
      "Requirement already satisfied: packaging in /software/opt/el_9/x86_64/python/3.11-2023.09/lib/python3.11/site-packages (from awkward>=2.0.0->awkward-pandas) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /software/opt/el_9/x86_64/python/3.11-2023.09/lib/python3.11/site-packages (from pandas>=1.2->awkward-pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /software/opt/el_9/x86_64/python/3.11-2023.09/lib/python3.11/site-packages (from pandas>=1.2->awkward-pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /software/opt/el_9/x86_64/python/3.11-2023.09/lib/python3.11/site-packages (from pandas>=1.2->awkward-pandas) (2023.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /software/opt/el_9/x86_64/python/3.11-2023.09/lib/python3.11/site-packages (from importlib-metadata>=4.13.0->awkward>=2.0.0->awkward-pandas) (3.16.2)\n",
      "Requirement already satisfied: six>=1.5 in /software/opt/el_9/x86_64/python/3.11-2023.09/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.2->awkward-pandas) (1.16.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install awkward-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uproot # for reading .root files\n",
    "import pandas as pd # to store data as dataframea\n",
    "import time # to measure time to analyse\n",
    "import math # for mathematical functions such as square root\n",
    "import numpy as np # # for numerical calculations such as histogramming\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from matplotlib.ticker import MaxNLocator,AutoMinorLocator # for minor ticks\n",
    "from lmfit.models import PolynomialModel, GaussianModel # for the signal and background fits\n",
    "import requests # for HTTP access\n",
    "import aiohttp # HTTP client support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check tree content of old and new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runNumber',\n",
       " 'eventNumber',\n",
       " 'channelNumber',\n",
       " 'mcWeight',\n",
       " 'scaleFactor_PILEUP',\n",
       " 'scaleFactor_ELE',\n",
       " 'scaleFactor_MUON',\n",
       " 'scaleFactor_PHOTON',\n",
       " 'scaleFactor_TAU',\n",
       " 'scaleFactor_BTAG',\n",
       " 'scaleFactor_LepTRIGGER',\n",
       " 'scaleFactor_PhotonTRIGGER',\n",
       " 'trigE',\n",
       " 'trigM',\n",
       " 'trigP',\n",
       " 'lep_n',\n",
       " 'lep_truthMatched',\n",
       " 'lep_trigMatched',\n",
       " 'lep_pt',\n",
       " 'lep_eta',\n",
       " 'lep_phi',\n",
       " 'lep_E',\n",
       " 'lep_z0',\n",
       " 'lep_charge',\n",
       " 'lep_type',\n",
       " 'lep_isTightID',\n",
       " 'lep_ptcone30',\n",
       " 'lep_etcone20',\n",
       " 'lep_trackd0pvunbiased',\n",
       " 'lep_tracksigd0pvunbiased',\n",
       " 'met_et',\n",
       " 'met_phi',\n",
       " 'jet_n',\n",
       " 'jet_pt',\n",
       " 'jet_eta',\n",
       " 'jet_phi',\n",
       " 'jet_E',\n",
       " 'jet_jvt',\n",
       " 'jet_trueflav',\n",
       " 'jet_truthMatched',\n",
       " 'jet_MV2c10',\n",
       " 'photon_n',\n",
       " 'photon_truthMatched',\n",
       " 'photon_trigMatched',\n",
       " 'photon_pt',\n",
       " 'photon_eta',\n",
       " 'photon_phi',\n",
       " 'photon_E',\n",
       " 'photon_isTightID',\n",
       " 'photon_ptcone30',\n",
       " 'photon_etcone20',\n",
       " 'photon_convType',\n",
       " 'tau_n',\n",
       " 'tau_pt',\n",
       " 'tau_eta',\n",
       " 'tau_phi',\n",
       " 'tau_E',\n",
       " 'tau_isTightID',\n",
       " 'tau_truthMatched',\n",
       " 'tau_trigMatched',\n",
       " 'tau_nTracks',\n",
       " 'tau_BDTid',\n",
       " 'ditau_m',\n",
       " 'lep_pt_syst',\n",
       " 'met_et_syst',\n",
       " 'jet_pt_syst',\n",
       " 'photon_pt_syst',\n",
       " 'tau_pt_syst',\n",
       " 'XSection',\n",
       " 'SumWeights',\n",
       " 'largeRjet_n',\n",
       " 'largeRjet_pt',\n",
       " 'largeRjet_eta',\n",
       " 'largeRjet_phi',\n",
       " 'largeRjet_E',\n",
       " 'largeRjet_m',\n",
       " 'largeRjet_truthMatched',\n",
       " 'largeRjet_D2',\n",
       " 'largeRjet_tau32',\n",
       " 'largeRjet_pt_syst',\n",
       " 'tau_charge']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# old file\n",
    "pnameo = 'https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/GamGam/Data/'+'data_A'+\".GamGam.root\"\n",
    "\n",
    "treeo = uproot.open(pnameo+':mini')\n",
    "\n",
    "treeo.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197327510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ScaleFactor_PILEUP',\n",
       " 'mcWeight',\n",
       " 'xsec',\n",
       " 'trigE',\n",
       " 'trigM',\n",
       " 'ScaleFactor_BTAG',\n",
       " 'jet_n',\n",
       " 'jet_pt',\n",
       " 'jet_eta',\n",
       " 'jet_phi',\n",
       " 'jet_e',\n",
       " 'jet_DL1d77_isBtagged',\n",
       " 'jet_jvt',\n",
       " 'largeRJet_n',\n",
       " 'largeRJet_pt',\n",
       " 'largeRJet_eta',\n",
       " 'largeRJet_phi',\n",
       " 'largeRJet_e',\n",
       " 'largeRJet_m',\n",
       " 'largeRJet_D2',\n",
       " 'ScaleFactor_ELE',\n",
       " 'ScaleFactor_MUON',\n",
       " 'lep_n',\n",
       " 'lep_type',\n",
       " 'lep_pt',\n",
       " 'lep_eta',\n",
       " 'lep_phi',\n",
       " 'lep_e',\n",
       " 'lep_charge',\n",
       " 'lep_ptvarcone30',\n",
       " 'lep_topoetcone20',\n",
       " 'lep_z0',\n",
       " 'lep_d0',\n",
       " 'lep_d0sig',\n",
       " 'lep_isTight',\n",
       " 'lep_isTightID',\n",
       " 'lep_isTightIso',\n",
       " 'ScaleFactor_PHOTON',\n",
       " 'photon_n',\n",
       " 'photon_pt',\n",
       " 'photon_eta',\n",
       " 'photon_phi',\n",
       " 'photon_e',\n",
       " 'photon_ptcone20',\n",
       " 'photon_topoetcone40',\n",
       " 'photon_isTight',\n",
       " 'photon_isTightID',\n",
       " 'photon_isTightIso',\n",
       " 'ScaleFactor_TAU',\n",
       " 'tau_n',\n",
       " 'tau_pt',\n",
       " 'tau_eta',\n",
       " 'tau_phi',\n",
       " 'tau_e',\n",
       " 'tau_charge',\n",
       " 'tau_nTracks',\n",
       " 'tau_isTight',\n",
       " 'tau_RNNJetScore',\n",
       " 'tau_RNNEleScore',\n",
       " 'met',\n",
       " 'met_phi',\n",
       " 'met_mpx',\n",
       " 'met_mpy']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new file\n",
    "pname = '/project/etp1/dkoch/ATLASOpenData/ntuples-data-samples/data15_allyear.root'\n",
    "tree = uproot.open(pname+':analysis')\n",
    "\n",
    "print(tree.num_entries)\n",
    "\n",
    "tree.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "myt = tree.iterate([\"photon_n\",\"photon_pt\",\"photon_eta\",\"photon_phi\",\"photon_e\",\n",
    "                            \"photon_isTightID\",\"photon_ptcone20\"], # add more variables here if you want to use them\n",
    "                           library=\"pd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = next(myt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.query('photon_n>=2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911256"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(data.photon_n)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911256\n"
     ]
    }
   ],
   "source": [
    "data.photon_n>=2\n",
    "num_survived_entries = (data['photon_n'] >= 2).sum()\n",
    "print(num_survived_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fraction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lumi, fraction, file path\n",
    "\n",
    "General definitions of luminosity, fraction of data used, where to access the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lumi = 0.5 # fb-1 # data_A only\n",
    "#lumi = 1.9 # fb-1 # data_B only\n",
    "#lumi = 2.9 # fb-1 # data_C only\n",
    "#lumi = 4.7 # fb-1 # data_D only\n",
    "lumi = 10 # fb-1 # data_A,data_B,data_C,data_D\n",
    "\n",
    "fraction = 0.8 # reduce this is you want the code to run quicker\n",
    "\n",
    "#tuple_path = \"Input/GamGam/Data/\" # local \n",
    "tuple_path = \"/project/etp1/dkoch/ATLASOpenData/ntuples-data-samples/\"\n",
    "#tuple_path = \"https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/GamGam/Data/\" # web address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='samples'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples\n",
    "\n",
    "Samples to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#samples_list = ['data_A','data_B','data_C','data_D'] # add if you want more data\n",
    "samples_list = ['data15_allyear', 'data16_allyear_A', 'data16_allyear_B', \n",
    "                'data16_allyear_C', 'data16_allyear_D', 'data16_allyear_E', \n",
    "                'data16_allyear_F', 'data16_allyear_G', 'data16_allyear_H' ]\n",
    "#samples_list = ['data16_allyear_A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to get data from files\n",
    "\n",
    "The datasets used in this notebook have already been filtered to include at least 2 photons per event, so that processing is quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_from_files():\n",
    "\n",
    "    frames = [] # define empty list to hold data\n",
    "    for val in samples_list: # loop over each file\n",
    "        fileString = tuple_path+val+\".root\" # file name to open\n",
    "        temp = read_file_new(fileString) # call the function read_file defined below\n",
    "        frames.append(temp) # append dataframe returned from read_file to list of dataframes\n",
    "    data = pd.concat(frames) # concatenate list of dataframes together into one dataframe\n",
    "    \n",
    "    return data # return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to calculate diphoton invariant mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_myy(photon_pt,photon_eta,photon_phi,photon_E):\n",
    "    # first photon is [0], 2nd photon is [1] etc\n",
    "    px_0 = photon_pt[0]*math.cos(photon_phi[0]) # x-component of photon[0] momentum\n",
    "    py_0 = photon_pt[0]*math.sin(photon_phi[0]) # y-component of photon[0] momentum\n",
    "    pz_0 = photon_pt[0]*math.sinh(photon_eta[0]) # z-component of photon[0] momentum\n",
    "    px_1 = photon_pt[1]*math.cos(photon_phi[1]) # x-component of photon[1] momentum\n",
    "    py_1 = photon_pt[1]*math.sin(photon_phi[1]) # y-component of photon[1] momentum\n",
    "    pz_1 = photon_pt[1]*math.sinh(photon_eta[1]) # z-component of photon[1] momentum\n",
    "    sumpx = px_0 + px_1 # x-component of diphoton momentum\n",
    "    sumpy = py_0 + py_1 # y-component of diphoton momentum\n",
    "    sumpz = pz_0 + pz_1 # z-component of diphoton momentum \n",
    "    sump = math.sqrt(sumpx**2 + sumpy**2 + sumpz**2) # magnitude of diphoton momentum \n",
    "    sumE = photon_E[0] + photon_E[1] # energy of diphoton system\n",
    "    m2 = sumE**2 - sump**2\n",
    "    if m2>0.:\n",
    "        m = m2**0.5\n",
    "    else:\n",
    "        print('calc_myy error neg mass**2', m2)\n",
    "        m = 0.\n",
    "    return m "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='changing_cut'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing a cut\n",
    "\n",
    "If you change a cut: Cell -> Run All Below\n",
    "\n",
    "If you change a cut here, you also need to make sure the cut is applied in the \"[Applying a cut](#applying_cut)\" cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cut on photon reconstruction quality\n",
    "def cut_photon_reconstruction(photon_isTightID):\n",
    "    initial_count = len(photon_isTightID)\n",
    "    # Assuming photon_isTightID is a DataFrame with two boolean columns\n",
    "    mask = (photon_isTightID['col1'] == True) & (photon_isTightID['col2'] == True)  # Adjust column names accordingly\n",
    "    final_count = mask.sum()\n",
    "    cut_count = initial_count - final_count\n",
    "    cut_percentage = (cut_count / initial_count) * 100\n",
    "    print(f\"1) Photon Reconstruction Cut: {initial_count} -> {final_count} events remaining \"\n",
    "          f\"({cut_count} events cut, {cut_percentage:.2f}% reduction)\")\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "# Cut on Transverse momentum\n",
    "def cut_photon_pt(photon_pt):\n",
    "    initial_count = len(photon_pt)\n",
    "    mask = (photon_pt[:, 0] > 40) & (photon_pt[:, 1] > 30)\n",
    "    final_count = np.sum(mask)\n",
    "    cut_count = initial_count - final_count\n",
    "    cut_percentage = (cut_count / initial_count) * 100\n",
    "    print(f\"2) Photon PT Cut: {initial_count} -> {final_count} events remaining \"\n",
    "          f\"({cut_count} events cut, {cut_percentage:.2f}% reduction)\")\n",
    "    return mask\n",
    "\n",
    "# Cut on energy isolation\n",
    "def cut_isolation_et(photon_ptcone20):\n",
    "    initial_count = len(photon_ptcone20)\n",
    "    mask = (photon_ptcone20[:, 0] < 4) & (photon_ptcone20[:, 1] < 4)\n",
    "    final_count = np.sum(mask)\n",
    "    cut_count = initial_count - final_count\n",
    "    cut_percentage = (cut_count / initial_count) * 100\n",
    "    print(f\"3) Isolation ET Cut: {initial_count} -> {final_count} events remaining \"\n",
    "          f\"({cut_count} events cut, {cut_percentage:.2f}% reduction)\")\n",
    "    return mask\n",
    "\n",
    "# Cut on pseudorapidity in barrel/end-cap transition region\n",
    "def cut_photon_eta_transition(photon_eta):\n",
    "    initial_count = len(photon_eta)\n",
    "    mask = ((np.abs(photon_eta[:, 0]) > 1.52) | (np.abs(photon_eta[:, 0]) < 1.37)) & \\\n",
    "           ((np.abs(photon_eta[:, 1]) > 1.52) | (np.abs(photon_eta[:, 1]) < 1.37))\n",
    "    final_count = np.sum(mask)\n",
    "    cut_count = initial_count - final_count\n",
    "    cut_percentage = (cut_count / initial_count) * 100\n",
    "    print(f\"4) Pseudorapidity Cut: {initial_count} -> {final_count} events remaining \"\n",
    "          f\"({cut_count} events cut, {cut_percentage:.2f}% reduction)\")\n",
    "    return mask\n",
    "\n",
    "# Cut on the number of photons\n",
    "#Alle photon_n sind >=2 schon.\n",
    "\n",
    "\n",
    "# Apply all cuts and summarize results\n",
    "def apply_cuts(photon_isTightID, photon_pt, photon_ptcone20, photon_eta, photon_n):\n",
    "    print(\"Applying Cuts:\")\n",
    "    n_in_total = len(photon_isTightID)\n",
    "    \n",
    "    # Apply each cut sequentially\n",
    "    reconstruction_mask = cut_photon_reconstruction(photon_isTightID)\n",
    "    pt_mask = cut_photon_pt(photon_pt)\n",
    "    isolation_mask = cut_isolation_et(photon_ptcone20)\n",
    "    eta_transition_mask = cut_photon_eta_transition(photon_eta)\n",
    "    photon_n_mask = cut_n_photon(photon_n)\n",
    "\n",
    "    # Combine all masks\n",
    "    final_mask = reconstruction_mask & pt_mask & isolation_mask & eta_transition_mask & photon_n_mask\n",
    "\n",
    "    # Final number of events after all cuts\n",
    "    n_out_total = np.sum(final_mask)\n",
    "    \n",
    "    print(\"\\nSummary of All Cuts:\")\n",
    "    print(f\"Total events before cuts: {n_in_total}\")\n",
    "    print(f\"Total events after all cuts: {n_out_total}\")\n",
    "    print(f\"Total events cut: {n_in_total - n_out_total} ({(n_in_total - n_out_total) / n_in_total * 100:.2f}% reduction)\")\n",
    "\n",
    "    return final_mask\n",
    "\n",
    "# Example usage with actual data arrays (replace with your actual data)\n",
    "# final_mask = apply_cuts(photon_isTightID, photon_pt, photon_ptcone20, photon_eta, photon_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values are True: True\n"
     ]
    }
   ],
   "source": [
    "# Apply the cut\n",
    "bool_series = data.photon_n >= 2\n",
    "\n",
    "# Check if all values are True\n",
    "all_true = bool_series.all()\n",
    "\n",
    "print(f\"All values are True: {all_true}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='applying_cut'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Applying a cut \n",
    "\n",
    "If you add a cut: Cell -> Run All Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pname = '/project/etp1/dkoch/ATLASOpenData/ntuples-data-samples/data15_allyear.root'\n",
    "#tree = uproot.open(pname+':analysis')\n",
    "def read_file_new(path):\n",
    "    start = time.time() # start the clock\n",
    "    print(\"Processing: \"+path) # print which sample is being processed\n",
    "    data_all = pd.DataFrame() # define empty pandas DataFrame to hold all data for this sample\n",
    "    tree = uproot.open(path + \":analysis\")\n",
    "    numevents = tree.num_entries # number of events\n",
    "    for data in tree.iterate([\"photon_n\",\"photon_pt\",\"photon_eta\",\"photon_phi\",\"photon_e\",\n",
    "                            \"photon_isTightID\",\"photon_ptcone20\"], # add more variables here if you want to use them\n",
    "                           library=\"pd\", # choose output type as pandas DataFrame\n",
    "                           entry_stop=numevents*fraction): # process up to numevents*fraction\n",
    "\n",
    "        nIn = len(data.index) # number of events in this batch\n",
    "        print(\"before cut \",len(data.index))\n",
    "        # Cut on # photons\n",
    "        #data = data[ np.vectorize(cut_n_photon)((data.photon_n))]\n",
    "        #print(\"nphot-cut \",len(data.index))\n",
    "\n",
    "        # Cut on photon reconstruction quality using the function cut_photon_reconstruction defined above\n",
    "        data = data[ np.vectorize(cut_photon_reconstruction)(data.photon_isTightID)]\n",
    "        \n",
    "        print(\"istight-cut \",len(data.index))\n",
    "        \n",
    "        # Cut on transverse momentum of the photons using the function cut_photon_pt defined above\n",
    "        data = data[ np.vectorize(cut_photon_pt)(data.photon_pt)]\n",
    "        print(\"pt-cut \",len(data.index))\n",
    "        \n",
    "        # Cut on energy isolation using the function cut_isolation_et defined above\n",
    "        data = data[ np.vectorize(cut_isolation_et)(data.photon_ptcone20)]\n",
    "        print(\"isol-cut \",len(data.index))\n",
    "        \n",
    "        # Cut on pseudorapidity inside barrel/end-cap transition region using the function cut_photon_eta_transition\n",
    "        data = data[ np.vectorize(cut_photon_eta_transition)(data.photon_eta)]\n",
    "        print(\"eta-cut \",len(data.index))\n",
    "        \n",
    "        # Calculate reconstructed diphoton invariant mass using the function calc_myy defined above\n",
    "        data['myy'] = np.vectorize(calc_myy)(data.photon_pt,data.photon_eta,data.photon_phi,data.photon_e)\n",
    "        \n",
    "        # dataframe contents can be printed at any stage like this\n",
    "        #print(data)\n",
    "\n",
    "        # dataframe column can be printed at any stage like this\n",
    "        #print(data['photon_pt'])\n",
    "\n",
    "        # multiple dataframe columns can be printed at any stage like this\n",
    "        #print(data[['photon_pt','photon_eta']])\n",
    "\n",
    "        nOut = len(data.index) # number of events passing cuts in this batch\n",
    "        data_all = pd.concat([data_all, data], ignore_index=True)\n",
    "        elapsed = time.time() - start # time taken to process\n",
    "        print(\"\\t nIn: \"+str(nIn)+\",\\t nOut: \\t\"+str(nOut)+\"\\t in \"+str(round(elapsed,1))+\"s\") # events before and after\n",
    "    \n",
    "    return data_all # return dataframe containing events passing all cuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the processing happens (this will take some minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /project/etp1/dkoch/ATLASOpenData/ntuples-data-samples/data16_allyear_A.root\n",
      "before cut  1826909\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[199], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#pname = '/project/etp1/dkoch/ATLASOpenData/ntuples-data-samples/data15_allyear.root'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;66;03m# time at start of whole processing\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mget_data_from_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# process all files\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#data = read_file_new(pname) #\u001b[39;00m\n\u001b[1;32m      5\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start \u001b[38;5;66;03m# time after whole processing\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[62], line 6\u001b[0m, in \u001b[0;36mget_data_from_files\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m samples_list: \u001b[38;5;66;03m# loop over each file\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     fileString \u001b[38;5;241m=\u001b[39m tuple_path\u001b[38;5;241m+\u001b[39mval\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.root\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# file name to open\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[43mread_file_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileString\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# call the function read_file defined below\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(temp) \u001b[38;5;66;03m# append dataframe returned from read_file to list of dataframes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(frames) \u001b[38;5;66;03m# concatenate list of dataframes together into one dataframe\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[198], line 21\u001b[0m, in \u001b[0;36mread_file_new\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore cut \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mindex))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Cut on # photons\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#data = data[ np.vectorize(cut_n_photon)((data.photon_n))]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#print(\"nphot-cut \",len(data.index))\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Cut on photon reconstruction quality using the function cut_photon_reconstruction defined above\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m data \u001b[38;5;241m=\u001b[39m data[ \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcut_photon_reconstruction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphoton_isTightID\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mistight-cut \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mindex))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Cut on transverse momentum of the photons using the function cut_photon_pt defined above\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/numpy/lib/function_base.py:2372\u001b[0m, in \u001b[0;36mvectorize.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_stage_2(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 2372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_as_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/numpy/lib/function_base.py:2365\u001b[0m, in \u001b[0;36mvectorize._call_as_normal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m     vargs \u001b[38;5;241m=\u001b[39m [args[_i] \u001b[38;5;28;01mfor\u001b[39;00m _i \u001b[38;5;129;01min\u001b[39;00m inds]\n\u001b[1;32m   2363\u001b[0m     vargs\u001b[38;5;241m.\u001b[39mextend([kwargs[_n] \u001b[38;5;28;01mfor\u001b[39;00m _n \u001b[38;5;129;01min\u001b[39;00m names])\n\u001b[0;32m-> 2365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vectorize_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/numpy/lib/function_base.py:2450\u001b[0m, in \u001b[0;36mvectorize._vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2448\u001b[0m     res \u001b[38;5;241m=\u001b[39m func()\n\u001b[1;32m   2449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2450\u001b[0m     ufunc, otypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ufunc_and_otypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2452\u001b[0m     \u001b[38;5;66;03m# Convert args to object arrays first\u001b[39;00m\n\u001b[1;32m   2453\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [asanyarray(a, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/numpy/lib/function_base.py:2410\u001b[0m, in \u001b[0;36mvectorize._get_ufunc_and_otypes\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcannot call `vectorize` on size 0 inputs \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2407\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munless `otypes` is set\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2409\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [arg\u001b[38;5;241m.\u001b[39mflat[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m-> 2410\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2412\u001b[0m \u001b[38;5;66;03m# Performance note: profiling indicates that -- for simple\u001b[39;00m\n\u001b[1;32m   2413\u001b[0m \u001b[38;5;66;03m# functions at least -- this wrapping can almost double the\u001b[39;00m\n\u001b[1;32m   2414\u001b[0m \u001b[38;5;66;03m# execution time.\u001b[39;00m\n\u001b[1;32m   2415\u001b[0m \u001b[38;5;66;03m# Hence we make it optional.\u001b[39;00m\n\u001b[1;32m   2416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache:\n",
      "Cell \u001b[0;32mIn[196], line 5\u001b[0m, in \u001b[0;36mcut_photon_reconstruction\u001b[0;34m(photon_isTightID)\u001b[0m\n\u001b[1;32m      3\u001b[0m initial_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(photon_isTightID)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming photon_isTightID is a DataFrame with two boolean columns\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m mask \u001b[38;5;241m=\u001b[39m (\u001b[43mphoton_isTightID\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m&\u001b[39m (photon_isTightID\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m final_count \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m      7\u001b[0m cut_count \u001b[38;5;241m=\u001b[39m initial_count \u001b[38;5;241m-\u001b[39m final_count\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "#pname = '/project/etp1/dkoch/ATLASOpenData/ntuples-data-samples/data15_allyear.root'\n",
    "start = time.time() # time at start of whole processing\n",
    "data = get_data_from_files() # process all files\n",
    "#data = read_file_new(pname) #\n",
    "elapsed = time.time() - start # time after whole processing\n",
    "print(\"Time taken: \"+str(round(elapsed,1))+\"s\") # print total time taken to process every file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plotting'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "If you only want a make a change in the plot: Cell -> Run All Below\n",
    "\n",
    "Define function to plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data(data):   \n",
    "\n",
    "    xmin = 100 # GeV\n",
    "    xmax = 160 # GeV\n",
    "    step_size = 2 # GeV\n",
    "    \n",
    "    bin_edges = np.arange(start=xmin, # The interval includes this value\n",
    "                     stop=xmax+step_size, # The interval doesn't include this value\n",
    "                     step=step_size ) # Spacing between values\n",
    "    bin_centres = np.arange(start=xmin+step_size/2, # The interval includes this value\n",
    "                            stop=xmax+step_size/2, # The interval doesn't include this value\n",
    "                            step=step_size ) # Spacing between values\n",
    "\n",
    "    data_x,_ = np.histogram(data['myy'], \n",
    "                            bins=bin_edges ) # histogram the data\n",
    "    data_x_errors = np.sqrt( data_x ) # statistical error on the data\n",
    "\n",
    "    # data fit\n",
    "    polynomial_mod = PolynomialModel( 4 ) # 4th order polynomial\n",
    "    gaussian_mod = GaussianModel() # Gaussian\n",
    "    \n",
    "    # set initial guesses for the parameters of the polynomial model\n",
    "    # c0 + c1*x + c2*x^2 + c3*x^3 + c4*x^4\n",
    "    pars = polynomial_mod.guess(data_x, # data to use to guess parameter values\n",
    "                                x=bin_centres, c0=data_x.max(), c1=0,\n",
    "                                c2=0, c3=0, c4=0 )\n",
    "    \n",
    "    # set initial guesses for the parameters of the Gaussian model\n",
    "    pars += gaussian_mod.guess(data_x, # data to use to guess parameter values\n",
    "                               x=bin_centres, amplitude=100, \n",
    "                               center=125, sigma=2 )\n",
    "    \n",
    "    model = polynomial_mod + gaussian_mod # combined model\n",
    "    \n",
    "    # fit the model to the data\n",
    "    out = model.fit(data_x, # data to be fit\n",
    "                    pars, # guesses for the parameters\n",
    "                    x=bin_centres, weights=1/data_x_errors ) \n",
    "\n",
    "    # background part of fit\n",
    "    params_dict = out.params.valuesdict() # get the parameters from the fit to data\n",
    "    c0 = params_dict['c0'] # c0 of c0 + c1*x + c2*x^2 + c3*x^3 + c4*x^4\n",
    "    c1 = params_dict['c1'] # c1 of c0 + c1*x + c2*x^2 + c3*x^3 + c4*x^4\n",
    "    c2 = params_dict['c2'] # c2 of c0 + c1*x + c2*x^2 + c3*x^3 + c4*x^4\n",
    "    c3 = params_dict['c3'] # c3 of c0 + c1*x + c2*x^2 + c3*x^3 + c4*x^4\n",
    "    c4 = params_dict['c4'] # c4 of c0 + c1*x + c2*x^2 + c3*x^3 + c4*x^4\n",
    "    \n",
    "    # get the background only part of the fit to data\n",
    "    background = c0 + c1*bin_centres + c2*bin_centres**2 + c3*bin_centres**3 + c4*bin_centres**4\n",
    "\n",
    "    # data fit - background fit = signal fit\n",
    "    signal_x = data_x - background \n",
    "\n",
    "\n",
    "    # *************\n",
    "    # Main plot \n",
    "    # *************\n",
    "    plt.axes([0.1,0.3,0.85,0.65]) # left, bottom, width, height \n",
    "    main_axes = plt.gca() # get current axes\n",
    "    \n",
    "    # plot the data points\n",
    "    main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors, \n",
    "                       fmt='ko', # 'k' means black and 'o' means circles\n",
    "                       label='Data' ) \n",
    "    \n",
    "    # plot the signal + background fit\n",
    "    main_axes.plot(bin_centres, # x\n",
    "                   out.best_fit, # y\n",
    "                   '-r', # single red line\n",
    "                   label='Sig+Bkg Fit ($m_H=125$ GeV)' )\n",
    "    \n",
    "    # plot the background only fit\n",
    "    main_axes.plot(bin_centres, # x\n",
    "                   background, # y\n",
    "                   '--r', # dashed red line\n",
    "                   label='Bkg (4th order polynomial)' )\n",
    "\n",
    "    # set the x-limit of the main axes\n",
    "    main_axes.set_xlim( left=xmin, right=xmax ) \n",
    "    \n",
    "    # separation of x-axis minor ticks\n",
    "    main_axes.xaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "    \n",
    "    # set the axis tick parameters for the main axes\n",
    "    main_axes.tick_params(which='both', # ticks on both x and y axes\n",
    "                          direction='in', # Put ticks inside and outside the axes\n",
    "                          top=True, # draw ticks on the top axis\n",
    "                          labelbottom=False, # don't draw tick labels on bottom axis\n",
    "                          right=True ) # draw ticks on right axis\n",
    "    \n",
    "    # write y-axis label for main axes\n",
    "    main_axes.set_ylabel('Events / '+str(step_size)+' GeV', \n",
    "                         horizontalalignment='right') \n",
    "    \n",
    "    # set the y-axis limit for the main axes\n",
    "    main_axes.set_ylim( bottom=0, top=np.amax(data_x)*1.1 ) \n",
    "    \n",
    "    # set minor ticks on the y-axis of the main axes\n",
    "    main_axes.yaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "    \n",
    "    # avoid displaying y=0 on the main axes\n",
    "    main_axes.yaxis.get_major_ticks()[0].set_visible(False) \n",
    "\n",
    "    # Add text 'ATLAS Open Data' on plot\n",
    "    plt.text(0.2, # x\n",
    "             0.92, # y\n",
    "             'ATLAS Open Data', # text\n",
    "             transform=main_axes.transAxes, # coordinate system used is that of main_axes\n",
    "             fontsize=13 ) \n",
    "    \n",
    "    # Add text 'for education' on plot\n",
    "    plt.text(0.2, # x\n",
    "             0.86, # y\n",
    "             'for education', # text\n",
    "             transform=main_axes.transAxes, # coordinate system used is that of main_axes\n",
    "             style='italic',\n",
    "             fontsize=8 ) \n",
    "    \n",
    "    # Add energy and luminosity\n",
    "    lumi_used = str(lumi*fraction) # luminosity to write on the plot\n",
    "    plt.text(0.2, # x\n",
    "             0.8, # y\n",
    "             '$\\sqrt{s}$=13 TeV,$\\int$L dt = '+lumi_used+' fb$^{-1}$', # text\n",
    "             transform=main_axes.transAxes ) # coordinate system used is that of main_axes \n",
    "    \n",
    "    # Add a label for the analysis carried out\n",
    "    plt.text(0.2, # x\n",
    "             0.74, # y\n",
    "             r'$H \\rightarrow \\gamma\\gamma$', # text \n",
    "             transform=main_axes.transAxes ) # coordinate system used is that of main_axes\n",
    "\n",
    "    # draw the legend\n",
    "    main_axes.legend(frameon=False, # no box around the legend\n",
    "                     loc='lower left' ) # legend location \n",
    "\n",
    "\n",
    "    # *************\n",
    "    # Data-Bkg plot \n",
    "    # *************\n",
    "    plt.axes([0.1,0.1,0.85,0.2]) # left, bottom, width, height\n",
    "    sub_axes = plt.gca() # get the current axes\n",
    "    \n",
    "    # set the y axis to be symmetric about Data-Background=0\n",
    "    sub_axes.yaxis.set_major_locator( MaxNLocator(nbins='auto', \n",
    "                                                  symmetric=True) )\n",
    "    \n",
    "    # plot Data-Background\n",
    "    sub_axes.errorbar(x=bin_centres, y=signal_x, yerr=data_x_errors,\n",
    "                      fmt='ko' ) # 'k' means black and 'o' means circles\n",
    "    \n",
    "    # draw the fit to data\n",
    "    sub_axes.plot(bin_centres, # x\n",
    "                  out.best_fit-background, # y\n",
    "                  '-r' ) # single red line\n",
    "    \n",
    "    # draw the background only fit\n",
    "    sub_axes.plot(bin_centres, # x\n",
    "                  background-background, # y\n",
    "                  '--r' )  # dashed red line\n",
    "    \n",
    "    # set the x-axis limits on the sub axes\n",
    "    sub_axes.set_xlim( left=xmin, right=xmax ) \n",
    "    \n",
    "    # separation of x-axis minor ticks\n",
    "    sub_axes.xaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "    \n",
    "    # x-axis label\n",
    "    sub_axes.set_xlabel(r'di-photon invariant mass $\\mathrm{m_{\\gamma\\gamma}}$ [GeV]',\n",
    "                        x=1, horizontalalignment='right', \n",
    "                        fontsize=13 ) \n",
    "    \n",
    "    # set the tick parameters for the sub axes\n",
    "    sub_axes.tick_params(which='both', # ticks on both x and y axes\n",
    "                         direction='in', # Put ticks inside and outside the axes\n",
    "                         top=True, # draw ticks on the top axis\n",
    "                         right=True ) # draw ticks on right axis \n",
    "    \n",
    "    # separation of y-axis minor ticks\n",
    "    sub_axes.yaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "    \n",
    "    # y-axis label on the sub axes\n",
    "    sub_axes.set_ylabel( 'Events-Bkg' ) \n",
    "\n",
    "\n",
    "    # Generic features for both plots\n",
    "    main_axes.yaxis.set_label_coords( -0.09, 1 ) # x,y coordinates of the y-axis label on the main axes\n",
    "    sub_axes.yaxis.set_label_coords( -0.09, 0.5 ) # x,y coordinates of the y-axis label on the sub axes\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function to plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='going_further'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can you do to explore this analysis?\n",
    "\n",
    "* Increase the fraction of data used in '[Lumi, fraction, file path](#fraction)'\n",
    "* Use data_B, data_C and data_D in '[Samples](#samples)'\n",
    "* Check how many events are being thrown away by each cut in '[Applying a cut](#applying_cut)'\n",
    "* Add more cuts from the [Higgs discovery paper](https://www.sciencedirect.com/science/article/pii/S037026931200857X#se0090) in '[Changing a cut](#changing_cut)' and '[Applying a cut](#applying_cut)'\n",
    "* Find the reduced chi-squared for the fit in '[Plotting](#plotting)'\n",
    "* Find the mean of the fitted Gaussian in '[Plotting](#plotting)'\n",
    "* Find the width of the fitted Gaussian in '[Plotting](#plotting)'\n",
    "* Try different initial guesses for the parameters of the fit in '[Plotting](#plotting)'\n",
    "* Try different functions for the fit in '[Plotting](#plotting)'\n",
    "* Your idea!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fitout_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data(data):   \n",
    "    # Define your range, binning, and calculate the histogram\n",
    "    xmin = 100 # GeV\n",
    "    xmax = 160 # GeV\n",
    "    step_size = 3 # GeV\n",
    "    \n",
    "    bin_edges = np.arange(start=xmin, stop=xmax + step_size, step=step_size)\n",
    "    bin_centres = np.arange(start=xmin + step_size/2, stop=xmax + step_size/2, step=step_size)\n",
    "\n",
    "    data_x, _ = np.histogram(data['myy'], bins=bin_edges)\n",
    "    data_x_errors = np.sqrt(data_x)  # Statistical error on the data\n",
    "\n",
    "    # Define the polynomial and Gaussian models\n",
    "    polynomial_mod = PolynomialModel(4)  # 4th order polynomial\n",
    "    gaussian_mod = GaussianModel()\n",
    "\n",
    "    # Initial parameter guesses\n",
    "    pars = polynomial_mod.guess(data_x, x=bin_centres, c0=data_x.max(), c1=0, c2=0, c3=0, c4=0)\n",
    "    pars += gaussian_mod.guess(data_x, x=bin_centres, amplitude=100, center=125, sigma=2)\n",
    "\n",
    "    # Combine the models\n",
    "    model = polynomial_mod + gaussian_mod\n",
    "\n",
    "    # Fit the model to the data\n",
    "    out = model.fit(data_x, pars, x=bin_centres, weights=1/data_x_errors)\n",
    "\n",
    "    # Check if fit was successful\n",
    "    if out.success:\n",
    "        print(\"Fit was successful!\")\n",
    "    else:\n",
    "        print(\"Fit failed!\")\n",
    "\n",
    "    # Extract and print fit parameters\n",
    "    params_dict = out.params.valuesdict()  # Get the fitted parameter values as a dictionary\n",
    "    for param, value in params_dict.items():\n",
    "        print(f\"{param} = {value}\")\n",
    "    \n",
    "    # Optional: return the parameters for further use\n",
    "    return params_dict\n",
    "\n",
    "    # ... (rest of your plotting code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
